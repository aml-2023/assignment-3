{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGlqnTYqA_lp"
   },
   "source": [
    "## Table of contents:\n",
    "* [The Transformer Architecture](#transformer) - **9 Points + 1 Bonus Point**\n",
    "    * Understanding the Attention Mechanism\n",
    "    * Scaled Dot Product Attention - **1 Point**\n",
    "    * Multi-Head Attention - **2 Points**\n",
    "    * The Encoder - Decoder Block - **3 Points**\n",
    "    * Positional Encoding - **1 Point**\n",
    "    * Transformer Network - **2 Points**\n",
    "        * Bonus Question - **1 Point**\n",
    "* [Graph Attention Network](#gat) **6 Points + 1 Bonus Point**\n",
    "    * Understanding the Graph Attention Mechanism\n",
    "    * Attention Mechanism\n",
    "        * Bonus Question - **1 Point**\n",
    "    * Write the code for the raw Attention Scores - **2 Points**\n",
    "    * Write the code for the Graph Attention Layer - **2 Points**\n",
    "    * Encoder-Decoder Block\n",
    "    * GAT Network - **1 Point**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ld6F6GDJA_ls"
   },
   "source": [
    "## Initial Setup\n",
    "Run the following two cellls to sync with Google Drive only if you run from Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WhC7DfPAA_ls"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "re0oSK4SA_lt"
   },
   "outputs": [],
   "source": [
    "%cd /content/drive/MyDrive/YOURPATH/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "le48bnfJA_lt"
   },
   "source": [
    "## The Transformer Architecture <a class=\"anchor\" id=\"transformer\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GdyMOogLA_lu"
   },
   "source": [
    "This notebook serves as a comprehensive guide to the fundamental components of the Transformer model, a highly influential architecture in deep learning. Since the release of the seminal paper by Vaswani et al. titled [Attention Is All You Need](https://arxiv.org/abs/1706.03762) in 2017, the Transformer design has consistently surpassed performance benchmarks, particularly in the field of natural language processing. Transformers equipped with a vast number of parameters have demonstrated the ability to generate extensive and compelling text, thus opening up new frontiers in AI applications.\n",
    "It is imperative to gain a thorough understanding of the inner workings of the Transformer architecture and to be able to implement it independently, a task we will accomplish within the context of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "y3nLcU1iSAey",
    "ExecuteTime": {
     "end_time": "2023-10-23T09:06:56.601488600Z",
     "start_time": "2023-10-23T09:06:23.742942500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import math\n",
    "import scipy.io\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "plt.set_cmap('cividis')\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "torch.backends.cudnn.deterministic=True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nh5tM4toA_lw"
   },
   "source": [
    "### Understanding the Attention Mechanism <a class=\"anchor\" id=\"att_mechanism\"></a>\n",
    "\n",
    "In recent years, particularly in sequence-related tasks, the attention mechanism has emerged as a crucial component within neural networks. This mechanism comprises a set of layers that have gained substantial attention due to their effectiveness. The primary purpose of the attention mechanism is to compute a weighted average of elements within a sequence. These weights are dynamically determined based on an input query and the keys associated with the elements. But what does this exactly entail?\n",
    "\n",
    "Essentially, the goal is to calculate an average that takes into account the true values of each element, rather than assigning equal weight to all. To achieve this, an attention mechanism typically consists of four key components:\n",
    "\n",
    "* **Query:** The query is a feature vector that helps identify specific elements within the sequence that require attention or consideration.\n",
    "\n",
    "* **Keys**: Each input element is associated with a key, which is also a feature vector. These keys provide insights into what each element contributes or when it becomes relevant. They are designed to enable the identification of elements that deserve attention based on the query.\n",
    "\n",
    "* **Values**: For each input element, there is a corresponding value vector. The aim is to compute an average using these value vectors.\n",
    "\n",
    "* **Score Function:** To determine the items deserving of attention, a scoring function denoted as $f_{attn}$ must be defined. This function takes the query and a key as inputs and yields both the attention weight and score for the query-key pair. Typically, common similarity metrics such as dot products or simple multi-layer perceptrons (MLPs) are employed for this purpose.\n",
    "\n",
    "***How are key (K), query (Q) and value (V) computed?***\n",
    "In these formulas, we'll denote the original representations as $(x_i)$ for each element in the sequence.\n",
    "\n",
    "1. **Key (K) Computation**:\n",
    "   - The key vector for each element $i$ is computed by multiplying the original representation $x_i$ by a learned key weight matrix $W^K$.\n",
    "   - Mathematically, the key vector $k_i$ is obtained as follows:\n",
    "\n",
    "     $k_i = x_i \\cdot W^K$\n",
    "\n",
    "2. **Query (Q) Computation**:\n",
    "   - Similarly, the query vector for each element $i$ is computed by multiplying the original representation $x_i$ by a learned query weight matrix $W^Q$.\n",
    "   - Mathematically, the query vector $q_i$ is obtained as follows:\n",
    "\n",
    "     $q_i = x_i \\cdot W^Q$\n",
    "\n",
    "3. **Value (V) Computation**:\n",
    "   - The value vector for each element $i$ is computed by multiplying the original representation $x_i$ by a learned value weight matrix $W^V$.\n",
    "   - Mathematically, the value vector $v_i$ is obtained as follows:\n",
    "\n",
    "     $v_i = x_i \\cdot W^V$\n",
    "\n",
    "Here's a bit more explanation:\n",
    "\n",
    "- $x_i$ represents the original representation (e.g., word embedding or feature vector) of the $i$-th element in the sequence.\n",
    "\n",
    "- $W^K$, $W^Q$, and $W^V$ are learnable weight matrices specific to the key, query, and value computations, respectively. These weight matrices are shared across all elements in the sequence but may have different dimensions based on the desired dimensionality of the key, query, and value spaces.\n",
    "\n",
    "- After computing the key, query, and value vectors for each element in the sequence, these vectors are used in the self-attention mechanism to calculate attention scores, which determine how much each element attends to others in the sequence. This process is typically followed by a weighted sum of the value vectors to obtain the final output for each element.\n",
    "\n",
    "To obtain the weights for averaging, a softmax function is applied to the scores produced by the scoring function across all elements. Consequently, value vectors associated with keys most similar to the query receive higher weights in the averaging process.\n",
    "\n",
    "$$\n",
    "\\alpha_i = \\frac{\\exp\\left(f_{attn}\\left(\\text{K}_i, \\text{Q}\\right)\\right)}{\\sum_j \\exp\\left(f_{attn}\\left(\\text{K}_j, \\text{Q}\\right)\\right)}, \\hspace{5mm} \\text{out} = \\sum_i \\alpha_i \\cdot \\text{V}_i\n",
    "$$\n",
    "\n",
    "Here is an example of attention over a sequence:\n",
    "\n",
    "<center width=\"100%\" style=\"padding:25px\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial6/attention_example.svg?raw=1\" width=\"750px\"></center>\n",
    "\n",
    "In this scenario, each word in the sequence has an associated key and value. The scoring function evaluates the similarity between the query and all the keys to determine the weights. These attention weights are then used to compute the weighted average of the word values.\n",
    "\n",
    "It's important to note that self-attention is a variant of attention applied within the Transformer architecture. In self-attention, each element in the sequence serves as both a key and a value and undergoes an attention layer. This layer assesses the similarity between the keys of all sequence elements based on the query of each element, ultimately producing unique averaged value vectors for each element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-qaBeNN7A_ly"
   },
   "source": [
    "### Scaled Dot Product Attention  (**1 Point**) <a class=\"anchor\" id=\"scaled_dot_product\"></a>\n",
    "\n",
    "The core concept behind self-attention is the scaled dot product attention, which aims to create an efficient attention mechanism that enables each element within a sequence to attend to every other element. This mechanism is designed to strike a balance between computational efficiency and expressive power.\n",
    "\n",
    "The inputs to the dot product attention consist of queries ($Q\\in\\mathbb{R}^{T\\times d_k}$), keys ($K\\in\\mathbb{R}^{T\\times d_k}$), and values ($V\\in\\mathbb{R}^{T\\times d_v}$). Here, $T$ represents the sequence length, while $d_k$ and $d_v$ denote the hidden dimensions for $Q$, $K$, and $V$.\n",
    "\n",
    "The dot product attention is computed as follows:\n",
    "\n",
    "$$\\text{Attention}(Q,K,V)=\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "The matrix multiplication $QK^T$ produces a matrix with dimension $T\\times T$ by doing the dot product for every distinct pair of queries and keys. The attention logits for a particular element $i$ to every other element in the sequence are shown in each row. We use a softmax on these and multiply by the value vector to get a weighted mean (the weights being determined by the attention). The computation graph below provides another viewpoint on this attention technique.\n",
    "\n",
    "<center width=\"100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial6/scaled_dot_product_attn.svg?raw=1\" width=\"210px\"></center>\n",
    "\n",
    "$1/\\sqrt{d_k}$, the scaling factor, is crucial to maintain an appropriate variance of attention values after initialization. As a result, $Q$ and $K$ may also have a variance of close to $1$.\n",
    "\n",
    "\n",
    "*Note: Keep in mind that we initialize our layers with the purpose of having equal variance across the model. Dot products over two vectors with variances $\\sigma^2$, however, produce scalars with $d_k$-times larger variance:*\n",
    "\n",
    "$$q_i \\sim \\mathcal{N}(0,\\sigma^2), k_i \\sim \\mathcal{N}(0,\\sigma^2) \\to \\text{Var}\\left(\\sum_{i=1}^{d_k} q_i\\cdot k_i\\right) = \\sigma^4\\cdot d_k$$\n",
    "\n",
    "\n",
    "*The optional masking of particular entries in the attention matrix is shown by the block labeled \"Mask (opt.)\" in the diagram above. When calculating the attention values, we pad the sentences to the same length and mask out the padding tokens.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Cv2BSG5A_l-"
   },
   "source": [
    "After the discussion regarding the scaled dot-product attention mechanism, please proceed to finalize the code for the `Attention` class as illustrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ZgYad7IHKEkY",
    "ExecuteTime": {
     "end_time": "2023-10-23T09:06:56.732490100Z",
     "start_time": "2023-10-23T09:06:56.611618200Z"
    }
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    ''' Scaled Dot-Product Attention '''\n",
    "\n",
    "    def __init__(self, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "\n",
    "        '''\n",
    "        add here the code regarding the argument of the softmax function as defined above\n",
    "        '''\n",
    "        # the dimensionality of the key vectors. it's the number of columns since the matrices are stacked row vectors.\n",
    "        key_dim = key.shape[1]\n",
    "\n",
    "        # compute attention, dot product between query and keys, divided by sqrt of the key vector dimensionality\n",
    "        attn = query @ key.T\n",
    "        attn /= torch.sqrt(torch.tensor(key_dim))\n",
    "\n",
    "\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attn = self.dropout(F.softmax(attn, dim=-1))\n",
    "\n",
    "        '''\n",
    "        Computed attn, calculate the final output of the attention layer\n",
    "        '''\n",
    "\n",
    "        # after the softmax, we just multiply by the value to get the output\n",
    "        output = attn @ value\n",
    "\n",
    "        return output, attn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjPYD5CNKEkZ"
   },
   "source": [
    "**Do not modify the code below.**\n",
    "\n",
    "After implementing the scaled dot-product attention mechanism, let's proceed with the completion of the `Attention` class below. For this initial implementation, we will not include the mask, which will be introduced and utilized in a subsequent step when building the `MultiHeadAttention` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qkt0l13yA_mA"
   },
   "source": [
    "Some random $Q$, $K$ and $V$ are generated to compute some attention outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PuV0ZKfKKEkZ",
    "outputId": "ce8bf659-95ba-4f62-f747-66104e47b03c",
    "ExecuteTime": {
     "end_time": "2023-10-23T09:06:57.171882200Z",
     "start_time": "2023-10-23T09:06:56.630745Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " tensor([[ 1.5410, -0.2934],\n",
      "        [-2.1788,  0.5684],\n",
      "        [-1.0845, -1.3986]])\n",
      "K\n",
      " tensor([[ 0.4033,  0.8380],\n",
      "        [-0.7193, -0.4033],\n",
      "        [-0.5966,  0.1820]])\n",
      "V\n",
      " tensor([[-0.8567,  1.1006],\n",
      "        [-1.0712,  0.1227],\n",
      "        [-0.5663,  0.3731]])\n",
      "Values\n",
      " tensor([[-0.9328,  0.8123],\n",
      "        [-0.9093,  0.3966],\n",
      "        [-0.9970,  0.3056]])\n",
      "Attention\n",
      " tensor([[0.6291, 0.2395, 0.2425],\n",
      "        [0.1387, 0.4749, 0.4975],\n",
      "        [0.0842, 0.6800, 0.3469]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "seq_len, d_k = 3, 2\n",
    "q = torch.randn(seq_len, d_k)\n",
    "k = torch.randn(seq_len, d_k)\n",
    "v = torch.randn(seq_len, d_k)\n",
    "attention = Attention()\n",
    "values, attn = attention(q, k, v)\n",
    "print(\"Q\\n\", q)\n",
    "print(\"K\\n\", k)\n",
    "print(\"V\\n\", v)\n",
    "print(\"Values\\n\", values)\n",
    "print(\"Attention\\n\", attn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWlZbDXSA_mB"
   },
   "source": [
    "### Multi-Head Attention  (**2 Points**) <a class=\"anchor\" id=\"multi_head\"></a>\n",
    "\n",
    "\n",
    "A network can effectively focus on various aspects of a sequence, thanks to the scaled dot product attention mechanism. However, for sequence elements, a single weighted average often falls short because they may need to consider multiple distinct characteristics. To address this limitation, we enhance the attention mechanism by introducing multiple heads, each equipped with its own set of query-key-value triplets applied to the same input features. Essentially, we transform a single query, key, and value matrix into $h$ sub-queries, sub-keys, and sub-values, and then independently process them through the scaled dot product attention. These individual head outputs are subsequently combined using a final weight matrix through concatenation.\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    \\text{Multihead}(Q,K,V) & = \\text{Concat}(\\text{head}_1,...,\\text{head}_h)W^{O}\\\\\n",
    "    \\text{where } \\text{head}_i & = \\text{Attention}(QW_i^Q,KW_i^K, VW_i^V)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "We refer to this as Multi-Head Attention layer. We can visually see it here:\n",
    "\n",
    "<center width=\"100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial6/multihead_attention.svg?raw=1\" width=\"230px\"></center>\n",
    "\n",
    "Set the feature map, $X\\in\\mathbb{R}^{B\\times T\\times d_{\\text{model}}}$, as $Q$, $K$ and $V$ (with $B$ as the batch size, $T$ the sequence length, $d_{\\text{model}}$ the hidden dimensionality of $X$). The weights $W^{Q}$, $W^{K}$, and $W^{V}$ can transform $X$ to the corresponding queries, keys, and values of the input. The final result is produced by multiplying the concatenated output by the weight matrix $W^{0}$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcxaSTkBA_mC"
   },
   "source": [
    "Complete the `MultiHeadAttention` class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "b8xqyzQRA_mC",
    "ExecuteTime": {
     "end_time": "2023-10-23T09:06:57.273630900Z",
     "start_time": "2023-10-23T09:06:57.181998700Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, d_model, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Take in model size and number of heads.\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        #  We assume d_v always equals d_k\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.query_ff = nn.Linear(d_model, d_model)\n",
    "        self.key_ff = nn.Linear(d_model, d_model)\n",
    "        self.value_ff = nn.Linear(d_model, d_model)\n",
    "        self.attn_ff = nn.Linear(d_model, d_model)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.attention = Attention(attn_dropout=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None, return_attention=False):\n",
    "\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k.\n",
    "        # The query is given as example, you should do the same for key and value\n",
    "        query = self.query_ff(query).view(nbatches, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        '''\n",
    "        Add your code below\n",
    "        '''\n",
    "        key = self.key_ff(key).view(nbatches, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        value = self.value_ff(value).view(nbatches, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        '''\n",
    "        Add your code below\n",
    "        '''\n",
    "        # we want to compute the attention for each head that we have.\n",
    "        # the 1st dimension is the batch dimension, since it is equal to 1, we just select it here\n",
    "        # TODO this probably isn't actually correct, what we if we have batch size > 1?\n",
    "        x = [self.attention.forward(query[0, head], key[0, head], value[0, head])[0] for head in range(num_heads)]\n",
    "\n",
    "        # put everything into a tensor so that we end up with size (num_heads, dim_q, dim_k) I think? TODO Double check dimensions\n",
    "        x = torch.stack(x)\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.num_heads * self.d_k)\n",
    "\n",
    "        if return_attention:\n",
    "            return self.attn_ff(x), self.attn\n",
    "\n",
    "        return self.attn_ff(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-7PDPT5A_mC"
   },
   "source": [
    "**Do not change the following code.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "RunMTxMlA_mC",
    "outputId": "cd3c42cf-7d02-4250-e914-1174f75c7c0b",
    "ExecuteTime": {
     "end_time": "2023-10-23T09:06:57.449835500Z",
     "start_time": "2023-10-23T09:06:57.232412600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-0.0423, -0.2251,  0.2807,  ...,  0.0061, -0.1711, -0.1070],\n         [-0.1002,  0.0772, -0.1022,  ..., -0.0416,  0.0872,  0.0043],\n         [-0.0089, -0.1227, -0.1472,  ...,  0.1748,  0.4361,  0.1108],\n         ...,\n         [-0.1033,  0.1601, -0.2226,  ...,  0.1035,  0.0179,  0.0989],\n         [ 0.1590,  0.0056, -0.1169,  ...,  0.1753, -0.0628, -0.1584],\n         [ 0.4620,  0.0152, -0.1945,  ...,  0.0325, -0.1405, -0.2588]]],\n       grad_fn=<ViewBackward0>)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "num_heads = 8\n",
    "d_model = 512\n",
    "\n",
    "\n",
    "self_attn = MultiHeadAttention(num_heads, d_model)\n",
    "\n",
    "\n",
    "x = torch.tensor(np.random.rand(1, 7,512)).float()\n",
    "\n",
    "attn_out = self_attn(x, x, x)\n",
    "attn_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kyqcnWVpA_mD"
   },
   "source": [
    "An essential feature of the multi-head attention mechanism is its **permutation-equivariance** concerning input elements, a critical aspect of this framework. In practical terms, if we were to interchange the first and second items within the input sequence, the output remains entirely unchanged. This property signifies that multi-head attention views the input not as a strict sequence but rather as a collection of items. It is this very characteristic that gives the Transformer architecture and the multi-head attention block their remarkable potency and versatility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9-c-5fHA_mD"
   },
   "source": [
    "### The Encoder-Decoder Block (**3 Points**) <a class=\"anchor\" id=\"encoder_decoder\"></a>\n",
    "\n",
    "The original Transformer model, as presented in the paper, was designed primarily for neural machine translation tasks, where it excels at translating sentences from one language to another, such as English to French. The key architectural concept used in the Transformer is the encoder-decoder architecture. In this setup, the encoder processes an input sentence, extracting meaningful features, which are then leveraged by the decoder to generate an output sentence, effectively performing translation.\n",
    "\n",
    "The completet Transformer architecture is illustrated below (figure credit - [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)).:\n",
    "\n",
    "<center width=\"100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial6/transformer_architecture.svg?raw=1\" width=\"400px\"></center>\n",
    "\n",
    "Let's examine the Encoder block more in depth. Understanding it will result in an easier comprehension of the Decoder block.\n",
    "\n",
    "\n",
    "The encoder is constructed by applying a sequence of identical blocks, denoted as $N$. Given an input $x$, the initial operation is the application of a Multi-Head Attention block. Subsequently, the output is augmented with the original input using a residual connection, and the sum is then normalized through a layer normalization. This process is formally represented as:\n",
    "\n",
    " $\\text{LayerNorm}(x+\\text{Multihead}(x,x,x))$ ($x$ being $Q$, $K$ and $V$ input to the attention layer).\n",
    "\n",
    "Residual connections are instrumental in ensuring a smooth gradient flow throughout the model and preserving vital information about the original sequence.\n",
    "\n",
    "Layer normalization serves multiple purposes—it accelerates training, provides a degree of regularization, and maintains consistent feature magnitudes across the sequence elements.\n",
    "\n",
    "Additionally, a small fully connected feed-forward network (FFN) is incorporated into the model, applied uniformly to each position. The transformation, inclusive of the residual connection, can be summarized as:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    \\text{FFN}(x) & = \\max(0, xW_1+b_1)W_2 + b_2\\\\\n",
    "    x & = \\text{LayerNorm}(x + \\text{FFN}(x))\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "To further enhance model robustness and prevent overfitting, dropout layers are strategically employed in the MLP, both on its output and in conjunction with the Multi-Head Attention as regularization measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4tE9P8VA_mD"
   },
   "source": [
    "Add your solution to the `EncoderBlock` and `DecoderBlock` classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "qsrr_V_cA_mD",
    "ExecuteTime": {
     "end_time": "2023-10-23T09:06:57.470961Z",
     "start_time": "2023-10-23T09:06:57.450826500Z"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            input_dim - Dimensionality of the input\n",
    "            num_heads - Number of heads to use in the attention block\n",
    "            dim_feedforward - Dimensionality of the hidden layer in the MLP\n",
    "            dropout - Dropout probability to use in the dropout layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Attention layer\n",
    "        self.self_attn = MultiHeadAttention(num_heads, input_dim)\n",
    "\n",
    "        # Two-layer MLP\n",
    "        self.linear_net = nn.Sequential(\n",
    "            nn.Linear(input_dim, dim_feedforward),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(dim_feedforward, input_dim)\n",
    "        )\n",
    "\n",
    "        # Layers to apply in between the main layers\n",
    "        self.norm1 = nn.LayerNorm(input_dim)\n",
    "        self.norm2 = nn.LayerNorm(input_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Self_attention part (use self.norm1)\n",
    "        multi_head_attn = self_attn.forward(x, x, x, mask=mask, return_attention=True)\n",
    "        normed = self.norm1(x + multi_head_attn)\n",
    "\n",
    "        # MLP part (use self.norm2)\n",
    "        ffn_out = self.linear_net.forward(normed)\n",
    "        normed = self.norm2(x + ffn_out)\n",
    "\n",
    "        x = normed\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hk8ozyFjA_mD"
   },
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            input_dim - Dimensionality of the input\n",
    "            num_heads - Number of heads to use in the attention block\n",
    "            dim_feedforward - Dimensionality of the hidden layer in the MLP\n",
    "            dropout - Dropout probability to use in the dropout layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Self Attention layer\n",
    "        self.self_attn = MultiHeadAttention(num_heads, input_dim)\n",
    "        # Attention Layer\n",
    "        self.src_attn = MultiHeadAttention(num_heads, input_dim)\n",
    "\n",
    "        # Two-layer MLP\n",
    "        self.linear_net = nn.Sequential(\n",
    "            nn.Linear(input_dim, dim_feedforward),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(dim_feedforward, input_dim)\n",
    "        )\n",
    "\n",
    "        # Layers to apply in between the main layers\n",
    "        self.norm1 = nn.LayerNorm(input_dim)\n",
    "        self.norm2 = nn.LayerNorm(input_dim)\n",
    "        self.norm3 = nn.LayerNorm(input_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        # Self-Attention part (use self.norm1)\n",
    "        '''\n",
    "        Add your code below\n",
    "        '''\n",
    "\n",
    "        # Attention part (use self.norm2)\n",
    "        # Recall that memory is the output of the encoder and replaces x as\n",
    "        # the key and value in the attention layer\n",
    "        '''\n",
    "        Add your code below\n",
    "        '''\n",
    "\n",
    "        # MLP part (use self.norm3)\n",
    "        '''\n",
    "        Add your code below\n",
    "        '''\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ke1i7zpNA_mE"
   },
   "source": [
    "### Positional Encoding  (**1 Point**)\n",
    "\n",
    "Positional information plays a vital role in tasks like language understanding, where the order of words in a sequence is crucial. To incorporate this positional context into our model, we can utilize positional encoding. Even if we were to learn embeddings for every possible position, it would not be feasible for sequences of varying lengths. Therefore, a more practical approach is to employ feature patterns that the network can discern from the input features and potentially generalize to longer sequences.\n",
    "\n",
    "Following the solution of Vaswani et al., the positional encoding is defined as:\n",
    "\n",
    "$$\n",
    "PE_{(pos,i)} = \\begin{cases}\n",
    "    \\sin\\left(\\frac{pos}{10000^{i/d_{\\text{model}}}}\\right) & \\text{if}\\hspace{3mm} i \\text{ mod } 2=0\\\\\n",
    "    \\cos\\left(\\frac{pos}{10000^{(i-1)/d_{\\text{model}}}}\\right) & \\text{otherwise}\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "In this equation, $PE_{(pos, i)}$ represents the positional encoding value at position $pos$ within the sequence and hidden dimension $i$. The combination of these values forms the positional information, which is added to the initial input features and concatenated across all hidden dimensions. This strategy allows the model to capture and utilize positional context effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wK9ueR5gKEkb"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Implement the PE function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        '''\n",
    "        Add your code below\n",
    "        '''\n",
    "        pe = pe.unsqueeze(0) # the final dimension is (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KVGi5FZoKEkb"
   },
   "source": [
    "To gain a deeper understanding of positional encoding, we can visualize it. We'll generate a sequence-based image that represents positional encoding across hidden dimensions. In this visualization, each pixel will signify the adjustment made to the input feature to encode a specific position.\n",
    "\n",
    "**Do not change the following code.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "id": "ey8R8ei5KEkb",
    "outputId": "abce9fe5-5498-4d3c-e20c-4ce02a950cca"
   },
   "outputs": [],
   "source": [
    "encod_block = PositionalEncoding(d_model=48, dropout=0.1, max_len=96)\n",
    "pe = encod_block.pe.squeeze().T.cpu().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,3))\n",
    "pos = ax.imshow(pe, cmap=\"RdGy\", extent=(1,pe.shape[1]+1,pe.shape[0]+1,1))\n",
    "fig.colorbar(pos, ax=ax)\n",
    "ax.set_xlabel(\"Position in sequence\")\n",
    "ax.set_ylabel(\"Hidden dimension\")\n",
    "ax.set_title(\"Positional encoding over hidden dimensions\")\n",
    "ax.set_xticks([1]+[i*10 for i in range(1,1+pe.shape[1]//10)])\n",
    "ax.set_yticks([1]+[i*10 for i in range(1,1+pe.shape[0]//10)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-OhCmxYKEkb"
   },
   "source": [
    "The sine and cosine waves with various wavelengths that encode the position in the hidden dimensions are easily visible. To better understand the pattern, we can examine the sine/cosine wave for each hidden dimension separately. The positional encoding for the hidden dimensions is shown in the image below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414
    },
    "id": "FnLQ-p4-KEkb",
    "outputId": "09471950-506e-41dc-9ea0-fca184e9ced3"
   },
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "fig, ax = plt.subplots(2, 2, figsize=(12,4))\n",
    "ax = [a for a_list in ax for a in a_list]\n",
    "for i in range(len(ax)):\n",
    "    ax[i].plot(np.arange(1,17), pe[i,:16], color=f'C{i}', marker=\"o\", markersize=6, markeredgecolor=\"black\")\n",
    "    ax[i].set_title(f\"Encoding in hidden dimension {i+1}\")\n",
    "    ax[i].set_xlabel(\"Position in sequence\", fontsize=10)\n",
    "    ax[i].set_ylabel(\"Positional encoding\", fontsize=10)\n",
    "    ax[i].set_xticks(np.arange(1,17))\n",
    "    ax[i].tick_params(axis='both', which='major', labelsize=10)\n",
    "    ax[i].tick_params(axis='both', which='minor', labelsize=8)\n",
    "    ax[i].set_ylim(-1.2, 1.2)\n",
    "fig.subplots_adjust(hspace=0.8)\n",
    "sns.reset_orig()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLI5sBXqA_mF"
   },
   "source": [
    "### Transformer Network  (**2 Points**)\n",
    "Everything we've talked about up to this point is summarized in the `Transformer` class. You will need all of the components (`EncoderBlock`, `DecoderBlock` and `PositionalEncoding`) previously seen to complete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GuZ4CCsaA_mF"
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, enc_inp_size, dec_inp_size, dec_out_size, N=6,\n",
    "                   d_model=512, dim_feedforward=2048, num_heads=8, dropout=0.1,\n",
    "                   mean=[0,0],std=[0,0]):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout = dropout\n",
    "        self.N = N\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.enc_inp_size = enc_inp_size\n",
    "        self.dec_inp_size = dec_inp_size\n",
    "        self.dec_out_size = dec_out_size\n",
    "\n",
    "        self.encoder = nn.ModuleList([deepcopy(\n",
    "            EncoderBlock(d_model, num_heads, dim_feedforward, dropout)) for _ in range(N)])\n",
    "        self.decoder = nn.ModuleList([deepcopy(\n",
    "            DecoderBlock(d_model, num_heads, dim_feedforward, dropout)) for _ in range(N)])\n",
    "        self.pos_enc = PositionalEncoding(d_model, dropout)\n",
    "        self.pos_dec = PositionalEncoding(d_model, dropout)\n",
    "        self.src_embed = nn.Linear(enc_inp_size, d_model)\n",
    "        self.tgt_embed = nn.Linear(dec_inp_size, d_model)\n",
    "        self.out = nn.Linear(d_model, dec_out_size)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "\n",
    "    def forward(self, src, trg, src_mask, trg_mask):\n",
    "\n",
    "        # First part of the forward pass: embedding and positional encoding\n",
    "        # both for the source and target\n",
    "        '''\n",
    "        Add your code below\n",
    "        '''\n",
    "\n",
    "        # Second part of the forward pass: the encoder and decoder layers.\n",
    "        # Look at the arguments of the forward pass of the encoder and decoder\n",
    "        # and recall that the encoder output is used as the memory in the decoder.\n",
    "        '''\n",
    "        Add your code below\n",
    "        '''\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    def init_weights(self):\n",
    "        for p in self.encoder.parameters():\n",
    "            if p.dim() > 1: nn.init.xavier_uniform_(p)\n",
    "        for p in self.decoder.parameters():\n",
    "            if p.dim() > 1: nn.init.xavier_uniform_(p)\n",
    "        for p in self.pos_enc.parameters():\n",
    "            if p.dim() > 1: nn.init.xavier_uniform_(p)\n",
    "        for p in self.pos_dec.parameters():\n",
    "            if p.dim() > 1: nn.init.xavier_uniform_(p)\n",
    "        for p in self.src_embed.parameters():\n",
    "            if p.dim() > 1: nn.init.xavier_uniform_(p)\n",
    "        for p in self.tgt_embed.parameters():\n",
    "            if p.dim() > 1: nn.init.xavier_uniform_(p)\n",
    "        for p in self.out.parameters():\n",
    "            if p.dim() > 1: nn.init.xavier_uniform_(p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LKmiKTH8A_mG"
   },
   "source": [
    "**Do not change the following code.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-kUgKm75A_mG"
   },
   "outputs": [],
   "source": [
    "# Select GPU device for the training if available\n",
    "if not torch.cuda.is_available():\n",
    "    device=torch.device(\"cpu\")\n",
    "    print(\"Current device:\", device)\n",
    "else:\n",
    "    device=torch.device(\"cuda\")\n",
    "    print(\"Current device:\", device, \"- Type:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "\n",
    "enc_input_size = 2\n",
    "dec_input_size = 3\n",
    "dec_output_size = 3\n",
    "\n",
    "\n",
    "num_heads = 8\n",
    "d_model = 512\n",
    "dim_feedforward = 2048\n",
    "dropout = 0.1\n",
    "preds_num = 8\n",
    "\n",
    "def subsequent_mask(size):\n",
    "    \"\"\"\n",
    "    Mask out subsequent positions.\n",
    "    \"\"\"\n",
    "    attn_shape = (1, size, size)\n",
    "    mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(mask) == 0\n",
    "\n",
    "torch.manual_seed(0)\n",
    "tf = Transformer(enc_input_size, dec_input_size, dec_output_size, N=6,\n",
    "            d_model=d_model, dim_feedforward=dim_feedforward,\n",
    "            num_heads=num_heads, dropout=dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0oHuAm6rA_mG"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "batch = torch.tensor(np.random.rand(1, 8,4)).float().to(device)\n",
    "inp = batch[:,1:,0:2].to(device)\n",
    "target = batch[:,:-1,2:].to(device)\n",
    "\n",
    "# We create a third mask channel to append to the 2 speeds.\n",
    "# This helps the decoder differentiating between start of sequence token (with mask token 1) and target speeds (with mask token 0)\n",
    "# Summarizing: start_of_seq token is (0,0) and the mask token is 1 ---> [0, 0, 1]\n",
    "#              target inputs are (u_i, v_i) and the mask token is 0 ---> [u_i, v_i, 0]\n",
    "start_of_seq = torch.Tensor([0, 0, 1]).unsqueeze(0).unsqueeze(1).repeat(target.shape[0], 1, 1).to(device)\n",
    "target_c = torch.zeros((target.shape[0], target.shape[1], 1)).to(device)\n",
    "target = torch.cat((target, target_c), -1)\n",
    "# Final decoder input is the concatenation of them along temporal dimension\n",
    "dec_inp = torch.cat((start_of_seq, target), 1)\n",
    "\n",
    "# Source attention is enabled between all the observed input (mask elements are setted to 1)\n",
    "src_att = torch.ones((inp.shape[0], 1, inp.shape[1])).to(device)\n",
    "# For the target attention we mask future elements to prevent model cheating (corresponding future mask elements are setted to False)\n",
    "# The mask is changed dinamically to use teacher forcing learning\n",
    "trg_att = subsequent_mask(dec_inp.shape[1]).repeat(dec_inp.shape[0], 1, 1).to(device)\n",
    "\n",
    "# Source, target and corresponding attention mask are passed to the model for the forward step\n",
    "tf.eval()\n",
    "pred = tf(inp.float(), dec_inp.float(), src_att, trg_att)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1r81ozQOA_mG"
   },
   "source": [
    "#### Bonus Question (**1 Point**)\n",
    "*Q: Considering the Recurrent Neural Network (RNN) architecture as the previous state-of-the-art for sequence modeling, what are the main advantages of the Transformer architecture?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCIZnh75A_mG"
   },
   "source": [
    "## Graph Attention Network Architecture <a class=\"anchor\" id=\"gat\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbJF1CT6A_mG"
   },
   "source": [
    "### Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUrBefHHA_mG"
   },
   "source": [
    "Graph neural networks (GNNs) constitute a robust category of neural networks designed for processing data organized in graph structures. They acquire node representations (embeddings) by gathering information from the nearby nodes of each individual node, a technique commonly referred to as 'message passing' within the realm of graph representation learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AgV2FB3vA_mc"
   },
   "source": [
    "![picture](https://drive.google.com/uc?export=view&id=1v68OlT2QLPDiKZXpCWQTExAG9wIYwdf0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ANyOlwwaA_mc"
   },
   "source": [
    "\n",
    "Throughout the GNN's numerous layers, these messages (embeddings) are communicated among nodes within the graph. At each layer, every node combines the messages it receives from its neighboring nodes to revise its own representation.\n",
    "\n",
    "\n",
    "![picture](https://drive.google.com/uc?export=view&id=1uyW6ZVN53uRHH5_UqUnRMOvpoQpz-ZOY)\n",
    "\n",
    "Ref. https://towardsai.net/p/machine-learning/graph-attention-networks-paper-explained-with-illustration-and-pytorch-implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRmhackeA_mc"
   },
   "source": [
    "### Understanding the Graph Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XrSxz8DkA_md"
   },
   "source": [
    "Graph Attention Networks (GAT) represent a groundbreaking advancement in the field of deep learning, particularly in the realm of graph neural networks (GNNs). GAT was introduced by Velickovic et al. in their 2017 paper titled [Graph Attention Networks](https://arxiv.org/abs/1710.10903). This innovative architecture has since become a pivotal tool for handling structured data represented as graphs, showcasing its effectiveness in various applications such as social network analysis, recommendation systems, and biology.\n",
    "\n",
    "GAT draws its inspiration from the Transformer architecture, which was initially developed for natural language processing tasks and introduced in the seminal paper \"Attention Is All You Need\" by Vaswani et al. in 2017. Just like Transformers revolutionized sequential data processing, GAT extends the principles of self-attention mechanisms to graphs, making it a powerful tool for modeling and processing structured data.\n",
    "\n",
    "Here are some key similarities between GAT and the Transformer architecture:\n",
    "\n",
    "1. **Self-Attention Mechanism**: Both GAT and Transformers employ self-attention mechanisms. In the case of Transformers, this mechanism allows the model to weigh the importance of different input tokens when generating an output. In GAT, self-attention is adapted to graph data, enabling nodes to attend to their neighbors with varying degrees of importance, capturing complex relationships within the graph.\n",
    "\n",
    "2. **Parallelization**: GAT, like Transformers, benefits from the inherent parallelizability of the self-attention mechanism. This enables efficient training and inference, making them suitable for handling large-scale data.\n",
    "\n",
    "3. **Flexibility**: Both architectures offer flexibility in modeling dependencies within the data. Transformers excel at capturing long-range dependencies in sequences, while GATs excel at capturing complex relationships between entities in a graph.\n",
    "\n",
    "4. **Scalability**: GATs can be scaled to handle graphs of different sizes, just as Transformers can be scaled to process sequences of varying lengths. This scalability is crucial for accommodating diverse real-world applications.\n",
    "\n",
    "5. **Multi-Head Attention**: Both GAT and Transformers can employ multi-head attention mechanisms, which enable the model to focus on different aspects of the data simultaneously. This enhances the model's ability to capture diverse and intricate patterns.\n",
    "\n",
    "In summary, Graph Attention Networks, inspired by the Transformer architecture, have ushered in a new era of deep learning for graph-structured data. These two architectures share fundamental principles, such as self-attention mechanisms and parallelization, while GAT specializes in leveraging these principles to handle graph data effectively, making it a valuable tool in various domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Ihwtt3mA_md"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import math\n",
    "import scipy.io\n",
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "torch.backends.cudnn.deterministic=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FsBwXIQrA_md"
   },
   "source": [
    "### Attention Mechanism\n",
    "\n",
    "1. **Step 1: Self-attention mechanism** for each node in the graph:\n",
    "\n",
    "   The attention mechanism computes attention coefficients for each node $i$ in the graph based on its neighbors. It does this by applying a shared self-attention mechanism to each edge $(i, j)$ in the graph, where $j$ is a neighboring node of $i$. The attention coefficients are computed as follows:\n",
    "\n",
    "   For each edge $(i, j)$:\n",
    "   \n",
    "   - Compute an unnormalized attention score $e_{ij}$ for the edge:\n",
    "     \n",
    "     $$e_{ij} = \\text{LeakyReLU}(a^T [W h_i, W  h_j])$$\n",
    "     \n",
    "     Here,\n",
    "     - $a$ is a learnable weight vector for the attention mechanism. In the original paper In our experiments, the attention mechanism a is a single-layer feedforward neural network,parametrized by a weight vector $a \\in \\mathbb{R}^{2F}$, where $F$ is the dimensionality of the node features.\n",
    "     - $W$ is a weight matrix that is shared for all edges.\n",
    "     - $h_i$ and $h_j$ are the node feature representations for nodes $i$ and $j$, respectively.\n",
    "     - $\\text{LeakyReLU}$ is a leaky rectified linear unit activation function.\n",
    "\n",
    "2. **Step 2: Attention coefficients normalization**:\n",
    "\n",
    "   The unnormalized attention scores are then normalized across all neighbors of node $i$ using the softmax function to obtain the final attention coefficients:\n",
    "   \n",
    "   $$\\alpha_{ij} = \\text{softmax}_j(e_{ij})$$\n",
    "   \n",
    "   Here, $N(i)$ represents the set of neighbors of node $i$.\n",
    "\n",
    "3. **Step 3: Aggregation of neighbor information**:\n",
    "\n",
    "   Once the attention coefficients are computed, the next step is to aggregate the information from neighboring nodes. This is done by taking a weighted sum of the neighbor node features using the attention coefficients:\n",
    "   \n",
    "   $$h_i' = \\sum_{j \\in N(i)} \\alpha_{ij} \\cdot h_j$$\n",
    "   \n",
    "   Here, $h_i'$ is the updated representation of node $i$.\n",
    "\n",
    "4. **Final output**:\n",
    "\n",
    "   Finally, the updated node representations $h_i'$ is passed through a feedforward neural network layer with activation functions to obtain the final output representations for the nodes in the graph.\n",
    "\n",
    "   $$h_i' = \\sigma(h_i')$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fMTbNu_A_md"
   },
   "source": [
    "#### Bonus Question (1 Point)\n",
    "*Q: Compared to the Transformer Network, why doesn't the GAT also have Key, Query and Value?*\n",
    "\n",
    "*A: In Graph Attention Networks (GAT), a variation of the attention mechanism is used that doesn't explicitly employ separate key, query, and value vectors as in traditional self-attention mechanisms like those found in the Transformer model. Instead, GAT directly computes attention coefficients (attention scores) based on the node embeddings.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBf0IodvA_me"
   },
   "source": [
    "### Write the code for the Raw Attention Scores (2 Points)\n",
    "\n",
    "Implement the attention mechanism in the `Raw_attention_scores` class.\n",
    "The forward function should return the raw attention scores $e_{ij}$ for each edge $(i, j)$ in the graph.\n",
    "Where $e_{ij}$:\n",
    "\n",
    "$$e_{ij} = \\text{LeakyReLU}(a^T [W h_i, W h_j])$$\n",
    "\n",
    "For the sake of the exercise we will take the $W h_i, W h_j$ for granted, so that you have to implement the rest of the equation.\n",
    "\n",
    "$$\\tilde{h_i} = W h_i$$\n",
    "\n",
    "$$\\tilde{h_j} = W h_j$$\n",
    "\n",
    "So that:\n",
    "\n",
    "$$e_{ij} = \\text{LeakyReLU}(a^T [\\tilde{h_i}, \\tilde{h_j}])$$\n",
    "\n",
    "Where $a$ is a learnable weight vector $a \\in \\mathbb{R}^{2F}$, where $F$ is the dimensionality of the node features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iHD8a5sfA_me"
   },
   "outputs": [],
   "source": [
    "class RawAttentionScores(nn.Module):\n",
    "    '''Defines the attention layer'''\n",
    "\n",
    "    def __init__(self, out_features=3, dropout=0.1, alpha=0.1, n_heads=1, concat=True):\n",
    "        super(RawAttentionScores, self).__init__()\n",
    "        self.out_features = out_features\n",
    "        self.dropout = dropout\n",
    "        self.alpha = alpha\n",
    "        self.n_heads = n_heads\n",
    "        self.n_hidden = out_features // n_heads if concat else out_features\n",
    "\n",
    "        self.a = nn.Parameter(torch.empty(size=(n_heads, 2*self.n_hidden, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "    def forward(self, h):\n",
    "\n",
    "        # Calculate attention coefficients using einsum https://pytorch.org/docs/stable/generated/torch.einsum.html\n",
    "        '''\n",
    "        Add your code below\n",
    "        '''\n",
    "        h_i_tilde = ...\n",
    "        h_j_tilde = ...\n",
    "\n",
    "        # Broadcasted addition https://pytorch.org/docs/stable/notes/broadcasting.html -> output shape (n_heads, n_nodes, n_nodes)\n",
    "        '''\n",
    "        Add your code below\n",
    "        '''\n",
    "        e = ...\n",
    "\n",
    "        # Activation function\n",
    "        attention_coefficients = F.leaky_relu(e, negative_slope=self.alpha)\n",
    "\n",
    "        return attention_coefficients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4SU9rgtNA_me"
   },
   "source": [
    "**Do not modify the code below.**\n",
    "\n",
    "We generate random values to test `Attention coefficients`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gA8nEtXxA_mf"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "in_features = 3\n",
    "number_of_nodes = 3\n",
    "n_heads = 1\n",
    "input = torch.rand(size=(n_heads, number_of_nodes, in_features))\n",
    "\n",
    "attention = RawAttentionScores()\n",
    "\n",
    "attention_coefficients = attention(input)\n",
    "\n",
    "print('Input:\\n ', input)\n",
    "print('Attention Scores:\\n ', attention_coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bLXZgo21A_mf"
   },
   "outputs": [],
   "source": [
    "def plot_graph(adj_matrix, labels=None):\n",
    "    G = nx.from_numpy_matrix(adj_matrix, create_using=nx.DiGraph)\n",
    "    pos = nx.spring_layout(G)\n",
    "\n",
    "\n",
    "    nx.draw_networkx_nodes(G, pos, cmap=plt.get_cmap('cividis'), node_size=500)\n",
    "\n",
    "    # Avoid edges id values is 0\n",
    "    edges = [(i, j) for i, j in G.edges if adj_matrix[i, j] != 0]\n",
    "\n",
    "    # Draw edges\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=edges, arrows=True, connectionstyle='arc3, rad = 0.1',\n",
    "                           edge_cmap=plt.get_cmap('cividis'), edge_color=adj_matrix.flatten()[np.flatnonzero(adj_matrix)])#edge_color=adj_matrix.flatten()[np.flatnonzero(adj_matrix)])\n",
    "\n",
    "\n",
    "    # Draw node labels\n",
    "    if labels is not None:\n",
    "        nx.draw_networkx_labels(G, pos, labels=labels)\n",
    "\n",
    "\n",
    "    # Add legend for edge colors\n",
    "    sm = plt.cm.ScalarMappable(cmap=plt.get_cmap('cividis'))\n",
    "    sm.set_array(adj_matrix.flatten())\n",
    "    plt.colorbar(sm)\n",
    "\n",
    "    plt.show()\n",
    "print(attention_coefficients)\n",
    "plot_graph((np.round(attention_coefficients.detach().numpy(),2))[0], labels={0: 'A', 1: 'B', 2: 'C'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HSA72C9A_mf"
   },
   "source": [
    "### Write the code for the Graph Attention Layer (2 Points)\n",
    "\n",
    "Now finalize the architecture by implementing the `GraphAttentionLayer` class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F9uO_jzFA_mg"
   },
   "outputs": [],
   "source": [
    "class GraphAttentionLayer(nn.Module):\n",
    "    def __init__(self, in_features=3, out_features=128, dropout=0.1, alpha=0.1, heads=4, concat=True):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.dropout = dropout\n",
    "        self.alpha = alpha\n",
    "        self.heads = heads\n",
    "        self.concat = concat\n",
    "\n",
    "        if concat: # concatenating the attention heads\n",
    "            assert out_features % self.heads == 0 # Ensure that out_features is a multiple of n_heads\n",
    "            self.n_hidden = out_features // self.heads\n",
    "        else: # averaging output over the attention heads (original)\n",
    "            self.n_hidden = out_features\n",
    "\n",
    "        self.W = nn.Parameter(torch.empty(size=(in_features, self.n_hidden * heads)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        self.a = nn.Parameter(torch.empty(size=(heads, 2*self.n_hidden, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        self.attention = RawAttentionScores(out_features, dropout, alpha,\n",
    "                                         n_heads=self.heads, concat=self.concat)\n",
    "\n",
    "\n",
    "    def forward(self, input, adj_matrix):\n",
    "        n_nodes = input.shape[0]\n",
    "\n",
    "        # Calculate out features trough matrix multiplication with einsum\n",
    "        h = torch.einsum('nc,cd->nd', input, self.W)\n",
    "        h = F.dropout(h, self.dropout, training=self.training)\n",
    "\n",
    "        # Divide h into heads\n",
    "        '''\n",
    "        Add your code below\n",
    "        '''\n",
    "\n",
    "\n",
    "        # Calculate attention scores using AttentionCoefficients\n",
    "        '''\n",
    "        Add your code below\n",
    "        '''\n",
    "        attention_coefficients = ...\n",
    "\n",
    "        # Mask attention scores\n",
    "        mask = -9e15*torch.ones_like(attention_coefficients)\n",
    "        attention_coefficients = torch.where(adj_matrix > 0, attention_coefficients, mask)\n",
    "\n",
    "        # Apply softmax\n",
    "        '''\n",
    "        Add your code below\n",
    "        '''\n",
    "        attention_scores = ...\n",
    "\n",
    "        # Calculate output features\n",
    "        h_prime = torch.einsum('hnd,hdo->hno', attention_scores, h)\n",
    "\n",
    "        # Concatenate heads\n",
    "        '''\n",
    "        Add your code below\n",
    "        '''\n",
    "\n",
    "\n",
    "        return h_prime, attention_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzD0afgSA_mg"
   },
   "source": [
    "**Do not modify the code below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LLOizDblA_mg"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "in_features = 3\n",
    "number_of_nodes = 3\n",
    "\n",
    "input = torch.rand(size=(number_of_nodes, in_features))\n",
    "adj_matrix = torch.tensor([[0, 1, 1], [1, 1, 0], [0, 1, 0]])\n",
    "\n",
    "attention = GraphAttentionLayer()\n",
    "\n",
    "output, attention_scores  = attention(input, adj_matrix)\n",
    "\n",
    "print('Output:\\n ', output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vjBs6EZ5A_mh"
   },
   "outputs": [],
   "source": [
    "for i in range(attention_scores.shape[0]):\n",
    "    print('Attention Scores:\\n ', attention_scores[i])\n",
    "    plot_graph((np.round(attention_scores.detach().numpy(),2))[i], labels={0: 'A', 1: 'B', 2: 'C'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_3_wa_dA_mh"
   },
   "source": [
    "### Encoder-Decoder Block\n",
    "\n",
    "Same as the Transformer, the GAT also uses the Encoder-Decoder architecture.\n",
    "**Do not modify the code below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iOglNZD0A_mh"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "        in_features=3,\n",
    "        out_features=128,\n",
    "        n_heads=4,\n",
    "        concat=True,\n",
    "        dropout=0.1,\n",
    "        leaky_relu_slope=0.2):\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.n_heads = n_heads\n",
    "        self.concat = concat\n",
    "        self.dropout = dropout\n",
    "        self.leaky_relu_slope = leaky_relu_slope\n",
    "\n",
    "        self.encoder = GraphAttentionLayer(in_features, out_features, dropout,\n",
    "                                           leaky_relu_slope, n_heads, concat)\n",
    "\n",
    "    def forward(self, input, adj_matrix):\n",
    "        output, _ = self.encoder(input, adj_matrix)\n",
    "        return F.elu(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rf1UH2yiA_mi"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "        in_features=3,\n",
    "        out_features=128,\n",
    "        n_heads=4,\n",
    "        concat=True,\n",
    "        dropout=0.1,\n",
    "        leaky_relu_slope=0.2):\n",
    "\n",
    "        super(Decoder, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.n_heads = n_heads\n",
    "        self.concat = concat\n",
    "        self.dropout = dropout\n",
    "        self.leaky_relu_slope = leaky_relu_slope\n",
    "\n",
    "        self.decoder = GraphAttentionLayer(in_features, out_features, dropout,\n",
    "                                           leaky_relu_slope, n_heads, concat)\n",
    "\n",
    "    def forward(self, input, adj_matrix):\n",
    "        output, _ = self.decoder(input, adj_matrix)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nbuv8XWJA_mi"
   },
   "source": [
    "### GAT Model (**1 Point**)\n",
    "\n",
    "Finally we can implement `GAT` model utilizing all of the classes built above.\n",
    "You will have to carefully read the code below to understand how the model is built, and instiantiate the `Encoder` and `Decoder` with the correct parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pj1qKF8SA_mi"
   },
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "    def __init__(self,\n",
    "        in_features=3,\n",
    "        n_hidden=128,\n",
    "        n_heads=4,\n",
    "        concat=True,\n",
    "        dropout=0.1,\n",
    "        leaky_relu_slope=0.2):\n",
    "\n",
    "        super(GAT, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_heads = n_heads\n",
    "        self.concat = concat\n",
    "        self.dropout = dropout\n",
    "        self.leaky_relu_slope = leaky_relu_slope\n",
    "\n",
    "        # Define encoder and decoder classes with the missing parameters\n",
    "        '''\n",
    "        Modify your code here\n",
    "        '''\n",
    "        # Note: the encoder uses multi-head attention, while the decoder uses single-head attention\n",
    "        self.encoder = Encoder(..., ..., n_heads, concat, dropout, leaky_relu_slope)\n",
    "        self.decoder = Decoder(..., ..., n_heads=1,\n",
    "                               concat=False, dropout=dropout, leaky_relu_slope=leaky_relu_slope)\n",
    "\n",
    "    def forward(self, input, adj_matrix):\n",
    "        output = self.encoder(input, adj_matrix)\n",
    "        output = self.decoder(output, adj_matrix)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NmY0Kq-iA_mi"
   },
   "source": [
    "**do not modify the code below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uSHvvAMlA_mi"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "in_features = 3\n",
    "number_of_nodes = 3\n",
    "\n",
    "gat = GAT(in_features=in_features, n_hidden=128, n_heads=4,\n",
    "          concat=True, dropout=0.1, leaky_relu_slope=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f7SXqaKZA_mj"
   },
   "outputs": [],
   "source": [
    "input = torch.rand(size=(number_of_nodes, in_features))\n",
    "adj_matrix = torch.tensor([[0, 1, 1], [1, 1, 0], [0, 0, 1]])\n",
    "\n",
    "output = gat(input.float(), adj_matrix.float())\n",
    "print('Input:\\n ', input)\n",
    "print('Output:\\n ', output)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.11.5 ('aml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "5655fe671c68e8ad27a1cda3205237718166dabff79966a668a86f5e057c4c5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
